{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard",
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/10100111/Display-of-HW1/blob/main/96_Light_%D0%A1%D0%B5%D1%82%D0%B8_%D1%81_%D0%B2%D0%BD%D0%B8%D0%BC%D0%B0%D0%BD%D0%B8%D0%B5%D0%BC_%D1%82%D0%B0%D0%B1%D0%BB%D0%B8%D1%86%D0%B0_%D1%80%D0%B5%D0%B7%D1%83%D0%BB%D1%8C%D1%82%D0%B0%D1%82%D0%BE%D0%B2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Задание Lite\n",
        "\n",
        "Макс 10 баллов\n",
        "\n",
        "Увеличьте словарный запас переводчика, изменив количество пар фраз поочередно на следующие значения:\n",
        "\n",
        "1. 60 000\n",
        "\n",
        "2. 80 000\n",
        "\n",
        "3. 100 000\n",
        "\n",
        "Постарайтесь достичь точности 0.05.\n",
        "\n",
        "Создайте таблицу эксперимента, в которую запишете точность и количество пар фраз.\n",
        "\n",
        "Напишите Ваши выводы.\n",
        "\n",
        "Разбор: Ноутбук: https://colab.research.google.com/drive/1Qmplw-hPb6shiNSIisvfMKsnQO_16OOg?usp=sharing"
      ],
      "metadata": {
        "id": "tYZ3N_8O7I3t"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Загрузка библиотек\n"
      ],
      "metadata": {
        "id": "PDsn3-BSAQnN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files # модуль для загрузки файлов в colab\n",
        "\n",
        "%tensorflow_version 2.x\n",
        "import tensorflow as tf\n",
        "\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer # загружаем токенайзер кераса для предобработки текстовыз данных\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences # метод для формирования послдовательностей одинаковой длины\n",
        "from tensorflow.keras.models import Model # загружаем абстрактный класс базовой модели сети\n",
        "from tensorflow.keras.layers import Dense, Embedding, GRU\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.losses import SparseCategoricalCrossentropy \n",
        "\n",
        "import numpy as np\n",
        "\n",
        "# Библиотеки для визуализации данных\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.ticker as ticker # модуль для определения форматирования и местоположения делений на осях графиков\n",
        "\n",
        "from sklearn.model_selection import train_test_split \n",
        "import re # модуль для работы с регулярными выражениями\n",
        "import time\n",
        "import os\n",
        "import gdown"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pzr4PSx2APm1",
        "outputId": "98af4588-8b7f-46bc-b587-6923afaa6a5a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Colab only includes TensorFlow 2.x; %tensorflow_version has no effect.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Загрузка и парсинг данных"
      ],
      "metadata": {
        "id": "Sdb6eIbFD-34"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "gdown.download('https://storage.yandexcloud.net/aiueducation/Content/advanced/l3/rus-eng.zip', None, quiet=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "a-OL9FAPn9Yj",
        "outputId": "b9baeb3e-29bc-4d0e-d536-d97eca50efa6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'rus-eng.zip'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 246
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip -o rus-eng.zip "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WqPc7VIAoRdj",
        "outputId": "51b49d4e-fe0d-4451-cc52-8668368f6349"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  rus-eng.zip\n",
            "  inflating: rus.txt                 \n",
            "  inflating: _about.txt              \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Проверим распакованные файлы\n",
        "!ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zIjYj_ieqADE",
        "outputId": "985911cd-1e33-4fe0-9708-200a685adf38"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "_about.txt  rus-eng.zip  rus.txt  sample_data\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Указываем путь к загруженному файлу\n",
        "path_to_file = \"/content/rus.txt\""
      ],
      "metadata": {
        "id": "blJpoR8nCqvZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Функции"
      ],
      "metadata": {
        "id": "Z10mQAfWv_tk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Предобработка предложений, очищаем от мусора, формируем нужную структуру слов и фраз\n",
        "def preprocess_sentence(phrases): # функция принимает содержимое словаря\n",
        "  # Разделяем пробелами слова и знаки препинания (\"А как насчет тебя? -> А как насчет тебя ? \")\n",
        "  phrases = re.sub(r\"([?.!,:;])\", r\" \\1 \", phrases) # r\" \\1 \" берет значения 1й группы в скобках; обрамляем указанные символы пробелами\"\n",
        "\n",
        "  # Заменяем все на пробелы, за исключением (a-zA-Za-яёА-ЯЁ?.!,;:)\n",
        "  phrases = re.sub(r\"[^a-zA-Za-яёА-ЯЁ?.!,;:]+\", \" \", phrases) # (а-zA-Za-яёА-ЯЁ) - английский и русский алфавит\n",
        "\n",
        "  phrases = phrases.rstrip().strip() # .rstrip() удаляем пробелы с конца строки = в конце фраз\n",
        "  phrases = \"<start> \" + phrases + \" <end>\" # обозначаем тегами начало и конец строки\n",
        "  return phrases # функция возвращает предобработанные фразы\n",
        "\n",
        "print(\"Фразы после обработки функцией с т.з. пунктуации примут вид: \")\n",
        "print(preprocess_sentence(\"What about you?\"))\n",
        "print(preprocess_sentence(\"А как на счет тебя?\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FNkit4vTC1mm",
        "outputId": "ea7dfae0-042c-44b0-d34d-e7c62e8eac4c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Фразы после обработки функцией с т.з. пунктуации примут вид: \n",
            "<start> What about you ? <end>\n",
            "<start> А как на счет тебя ? <end>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Функция создания датасета\n",
        "\n",
        "def create_dataset(path,          # Путь к файлу\n",
        "                   num_examples): # Необходимый размер датасета \n",
        "\n",
        "  # Открываем файл и разбиваем фразы на отдельные строчки\n",
        "  lines = open(path, encoding='UTF-8').read().strip().split('\\n')\n",
        "\n",
        "  # В каждой строке словаря разделяем английскую фразу от русской, и пропускаем через функцию предобработки данных\n",
        "  word_pairs = [[preprocess_sentence(phrases) for phrases in l.split('\\t')[0:2]]  for l in lines[:num_examples]]\n",
        "\n",
        "  # Вернем пары фраз в виде [по-английски, по-русски]\n",
        "  return zip(*word_pairs)"
      ],
      "metadata": {
        "id": "w5661daEK2Yj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Взглянем на пример пары фраз на выходе функции:\")\n",
        "\n",
        "english, russian = create_dataset(path_to_file,40000) # Вызовем функцию для демонстрации\n",
        "print(english[-1])                                    # Выведем последний элемент из списка английских фраз\n",
        "print(russian[-1])                                    # Выведем последний элемент из списка русских фраз"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TXC89L-nojhY",
        "outputId": "47f9648f-dc50-41d1-9c0c-f18381a981cf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Взглянем на пример пары фраз на выходе функции:\n",
            "<start> Don t be too long . <end>\n",
            "<start> Не тяните . <end>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Создадим функцию для получения максимльной длины фразы из списка. На вход принимает список фраз. Перебираем список, выбираем максимальное значение."
      ],
      "metadata": {
        "id": "uILX9m_Zq88V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Создадим мини-функцию, возвращающую максимальную длину тензора\n",
        "def max_length(tensor): # на вход принимает тензор (фразы в виде последовательности индексов)\n",
        "  return max(len(t) for t in tensor) # вернет значение максимальной длины его элемента"
      ],
      "metadata": {
        "id": "zDFXIxirqv4D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Функция преобразовывает тексты в последовательности индексов. Используем стандартный токенвайзер из модуля Керас. На вход принимает текст, обучаеь на нем токенайзер. Переводит текст в токеныю Отдает полученные токены и токенайзер. "
      ],
      "metadata": {
        "id": "VB0m_Oi6IHUe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Функция принимает текст одного из языков\n",
        "def tokenize(language):\n",
        "\n",
        "  language_tokenizer = Tokenizer(filters=\"\")         # вызываем класс токенизатора\n",
        "  language_tokenizer.fit_on_texts(language)          # скармливаем ему тексты для обработки и сборки словаря частотности\n",
        "  tensor = language_tokenizer.texts_to_sequences(language) # разбиваем текст фраз на последовательности индексов\n",
        "  tensor = pad_sequences(tensor, padding=\"post\")           # делаем последовательности одинаковой длины, заполняя нулями более короткие фразы\n",
        "\n",
        "  # Возвращаем последовательность индексов (назовем ее тензор) и токенизатор\n",
        "  return tensor, language_tokenizer"
      ],
      "metadata": {
        "id": "XPsKx7PxIeev"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Функция, формирующая готовый датасет, получает на вход путь к файлу с текстами и необходимый размер готового датасета."
      ],
      "metadata": {
        "id": "UlCDNL04NvMD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load_dataset(path, num_examples):\n",
        "\n",
        "  # Из исходного текста делаем датасет пар фраз, причем входным языком для НС сделаем русский\n",
        "  targ_language, inp_language = create_dataset(path, num_examples)\n",
        "\n",
        "  # Разбиваем текст на последовательность индексов (назовем ее тензор)\n",
        "  input_tensor, inp_language_tokenizer = tokenize(inp_language)    # формируем тезоры и токенайзер для русского языка\n",
        "  target_tensor, targ_language_tokenizer = tokenize(targ_language) # формируем тезоры и токенайзер для русского языка\n",
        "  \n",
        "  return input_tensor, target_tensor, inp_language_tokenizer, targ_language_tokenizer"
      ],
      "metadata": {
        "id": "7cGe2tRCKH6a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A6tkpocE6X_3"
      },
      "source": [
        "Посмотрим на примеры\n",
        "В первом блоке выведем русскую фразу и ее токен\n",
        "Во втором аглийскую.\n",
        "\n",
        "Далее выводим статистику по датасету"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Визуализируем собранные данные\n",
        "\n",
        "def convert(language_tokenizer,  # Токенайзер\n",
        "            tensor):             # Список индексов слов\n",
        "            \n",
        "  #  Цикл по токенам во фразе\n",
        "  for t in tensor:  \n",
        "    if t!=0:                                                        # Если токен не 0. Т.е. не мусор в конце фразы\n",
        "      print (\"%d ----> %s\" % (t, language_tokenizer.index_word[t])) # Выводи токен и соответствующее слово"
      ],
      "metadata": {
        "id": "P6X_dCeWaa03"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v2XeriQf5fVx"
      },
      "source": [
        "# print (\"Фраза на русском языке; соответствие индекса и слова\")   \n",
        "# convert(inp_language_tokenizer, input_tensor_train[0])           # Выведем нулевую пару из русского датасета\n",
        "# print ()    \n",
        "\n",
        "# print (\"Фраза на английском языке; соответствие индекса и слова\")\n",
        "# convert(targ_language_tokenizer, target_tensor_train[0])         # Выведем нулевую пару из агнлийского датасета\n",
        "# print ()   \n",
        "                                                      \n",
        "# print(\"Рус.яз. тренировочная: \" , len(input_tensor_train), \"фраз; \", \"Анг.яз. тренировочная: \", len(target_tensor_train), \"фраз\")# Выведем статистику по обучающей выборке\n",
        "# print(\"Рус.яз. тестовая: \", len(input_tensor_val), \"фраз; \", \"Анг.яз. тестовая: \", len(target_tensor_val), \"фраз\")               # Выведем статистику по тестовой выборке"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Объявление классов Encoder. Decoder, Attention"
      ],
      "metadata": {
        "id": "fa9iaHdZwXiR"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EZE7WSCKQ4-H"
      },
      "source": [
        "Вспомним нашу схему - сеть состоит из кодера, декодера и блока attention.\n",
        "\n",
        "Давайте начнем оформлять кодер в виде класса. В этом примере кодер состоит из блоков `Embedding` и `GRU`. Обратим внимание на `return_sequences=True`, `return_state=True` - мы требуем состояния кодера на каждом шаге работы. \n",
        "\n",
        "На вход принимает фразу для перевода и начальное состояние. Отдает выход GRU и вектор скрытых состояний"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TJwqiGRCDCu1"
      },
      "source": [
        "class Encoder(Model):\n",
        "\n",
        "  # Конструктор класса \n",
        "  def __init__(self, \n",
        "               vocab_size,    # Размер словаря\n",
        "               embedding_dim, # Размер пространсва эмбеддинга\n",
        "               enc_units,     # Число нейронов в GRU\n",
        "               batch_sz):     # Размер батча\n",
        "\n",
        "    super(Encoder, self).__init__()                                   # Даем возможность использовать и исполнять методы класса-родителя в классе потомке \n",
        "    self.batch_sz = batch_sz                                          # Атрибут возвращает размер батча\n",
        "    self.enc_units = enc_units                                        # Атрибут возвращает размер слоя в кодировщике\n",
        "    self.embedding = Embedding(vocab_size, embedding_dim)             # Атрибут эмбеддинга - слой Кераса с размером словаря на входе и с dim=256\n",
        "\n",
        "    # Реккурентной сетью выберем GRU, указываем размер слоя, вывод из слоя в виде последовательностей, \n",
        "    # и метод инициализации весов 'glorot_uniform'(или метод Ксавьера) для упрощения прохождения сигнала при распростр-ии ошибки\n",
        "    self.gru = GRU(self.enc_units, return_sequences=True, return_state=True, recurrent_initializer='glorot_uniform')\n",
        "\n",
        "  # Метод принимает входную фразу и начальное состояние\n",
        "  def call(self, \n",
        "           x,       # Входная фраза\n",
        "           hidden): # Начальное энкодера\n",
        "    x = self.embedding(x) # входящие тензоры преобразовываются в эмбеддинг\n",
        "    output, state = self.gru(x, initial_state = hidden) #затем пропускаются через GRU и получаем выход + новое состояние\n",
        "\n",
        "    # Выход сети GRU и состояние на выходе\n",
        "    return output, state \n",
        "\n",
        "  # Создаем метод инициализации состояний на скрытых слоях\n",
        "  def initialize_hidden_state(self):\n",
        "\n",
        "    # Вернем тензор из нулей размер батча на размер слоя, итсполбьзуем как начальное состояние энкодера\n",
        "    return tf.zeros((self.batch_sz, self.enc_units)) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1BoTUmLPSsha"
      },
      "source": [
        "Создаем экземпляр класса Encoder. Используем далее как готовый модуль при построении модели сети"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BgR4bQIKDFXF"
      },
      "source": [
        "# Создадим модель кодировщика по уже заданным параметрам \n",
        "encoder = Encoder(vocab_inp_size, embedding_dim, units, BATCH_SIZE)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eXH74Z-B_Mmb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "713936ea-2e44-4d4f-f720-f12d1ab77c7c"
      },
      "source": [
        "# Подадим в качестве примера какой-то сэмпл(Тензор[64, 12]) на вход Encoder'у и визуализируем, что получим\n",
        "sample_hidden = encoder.initialize_hidden_state() #инициализируем начальное скрытое состояние\n",
        "\n",
        "# Даем Encoder'у сэмпл и начальное состояние, и получим выход из сети GRU и состояние на выходе (вызывается метод call класса Encoder)\n",
        "sample_output, sample_hidden = encoder(example_input_batch, sample_hidden)\n",
        "print ('Размеры выхода из кодировщика: (batch size, sequence length, units) {}'.format(sample_output.shape))\n",
        "print ('Размеры скрытого состояния: (batch size, units) {}'.format(sample_hidden.shape))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Размеры выхода из кодировщика: (batch size, sequence length, units) (256, 15, 1024)\n",
            "Размеры скрытого состояния: (batch size, units) (256, 1024)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xKRwq1d-S86R"
      },
      "source": [
        "Создадим класс модуля `attenton`, как предписывал Bahdanau. Разбор работы данного модуля мы прошли чуть ранее. На входе состояния кодера `hidden_state` и `values` - выход предыдущего декодера с предыдущего шага. На выходе вектор контекста и веса `attention`:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VWDTK8eqDHu0"
      },
      "source": [
        "class BahdanauAttention(Model): # Название класса именем создателя механизма Дмитрия Богданова(Bahdanau)\n",
        "\n",
        "  # Создаем конструктор класса\n",
        "  def __init__(self, \n",
        "               units):                        # Число нейронов \n",
        "\n",
        "    super(BahdanauAttention, self).__init__() # Даем возможность использовать и исполнять методы класса-родителя в классе потомке\n",
        "    self.W1 = Dense(units)                    # Создаем Dense с заданным числом нейронов\n",
        "    self.W2 = Dense(units)                    # Создаем Dense с заданным числом нейронов\n",
        "    self.V =  Dense(1)                        # Создаем Dense с числом нейронов =1\n",
        "\n",
        "  # Метод принимает состояние и выход энкодера ----------------------------------\n",
        "  \n",
        "  def call(self, \n",
        "           hidden_state, # Состояние энкодера\n",
        "           values):      # Выход энкодера\n",
        "    # Форма состояния на скрытом слое (batch_size, hidden size)\n",
        "    # Форму состояния на каждом такте увеличим до (batch_size, 1, hidden size)\n",
        "    # Добавляем это для того, чтобы получить оценку\n",
        "    hidden_with_time_axis = tf.expand_dims(hidden_state, 1)\n",
        "\n",
        "    # Форма оценки score (размер батча, макс.длина слов на входе, 1), однёрка в конце, чтобы применить self.V\n",
        "    # До применения self.V оценка была бы (размер батча, макс.длина слов на входе, количество нейронов в слое)\n",
        "    score = self.V(tf.nn.tanh(self.W1(values) + self.W2(hidden_with_time_axis)))\n",
        "\n",
        "    # К полученной оценке применим Софтмакс, который покажет вероятность полезности от 0 до 1 для каждого слова в фразе для декодера\n",
        "    # Форма оценки score - (размер батча, макс.длина слов на входе, 1); Софтмакс применяем к оси \"макс.длина слов\"\n",
        "    attention_weights = tf.nn.softmax(score, axis=1)\n",
        "\n",
        "    # Построим вектор контекста \n",
        "    context_vector = attention_weights * values # Веса внимания перемножим со значениями(выхода из кодировщика)\n",
        "    # Сумму также применяем по оси \"макс.длина слов на входе\"\n",
        "    context_vector = tf.reduce_sum(context_vector, axis=1) # Размеры вектора контекста после суммирования будут (размер батча, размер слоя)\n",
        "\n",
        "    # Возвращает вектор контекста и веса внимания\n",
        "    return context_vector, attention_weights\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J8sMHv-wU_oD"
      },
      "source": [
        "Создадим экземпляр класса BahdanauAttention. Здесь 10 - число нейронов в первом dense слое"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hUv-DSsDDKGl",
        "outputId": "3c5324b4-1e29-4715-db07-6ad82cc59078"
      },
      "source": [
        "# Проверим, как работает слой\n",
        "attention_layer = BahdanauAttention(10)\n",
        "\n",
        "# Подадим на вход слою внимания выход из Encodera и его состояние, и получим значение и веса внимания\n",
        "attention_result, attention_weights = attention_layer(sample_hidden, sample_output)\n",
        "\n",
        "print(\"Размеры значения внимания: (размер батча, размер слоя) {}\".format(attention_result.shape))\n",
        "print(\"Размеры весов внимания: (размер батча, длина последовательности, 1) {}\".format(attention_weights.shape))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Размеры значения внимания: (размер батча, размер слоя) (256, 1024)\n",
            "Размеры весов внимания: (размер батча, длина последовательности, 1) (256, 15, 1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zK4api1fVM3N"
      },
      "source": [
        "Создаем класс декодера с attention. Декодер принимает обущающую фразу, прогоняет через embedding. Далее склеивает с вектором контента и подает на GRU.\n",
        "На выходе dense слой с числом нейронов равному размеру словаря."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gZHTEgDoDNBC"
      },
      "source": [
        "class Decoder(Model):\n",
        "\n",
        "  # Создадим конструктор класса\n",
        "  def __init__(self,   \n",
        "               vocab_size,    # Размер словаря\n",
        "               embedding_dim, # Размерность пространства эмбеддинга\n",
        "               dec_units,     # Число нейронов в GRU\n",
        "               batch_sz):     # Размер батча\n",
        "    super(Decoder, self).__init__()                       # Даем возможность использовать и исполнять методы класса-родителя в классе потомке \n",
        "    self.batch_sz = batch_sz                              # Атрибут возвращает размер батча\n",
        "    self.dec_units = dec_units                            # Атрибут возвращает размер слоя в декодере(кол-во нейронов)\n",
        "    self.embedding = Embedding(vocab_size, embedding_dim) # Атрибут эмбеддинга - слой Кераса с размером словаря на входе и (dim=256) на выходе\n",
        "\n",
        "    # Реккурентной сетью выберем GRU, указываем размер слоя, вывод из слоя в виде последовательностей, \n",
        "    # и метод инициализации весов 'glorot_uniform'(или метод Ксавьера) для упрощения прохождения сигнала при распростр-ии ошибки    \n",
        "    self.gru = GRU(self.dec_units, return_sequences=True, return_state=True, recurrent_initializer='glorot_uniform')\n",
        "\n",
        "    self.fc = Dense(vocab_size) # Атрибут вызовет полносвязный слой с размером словаря\n",
        "\n",
        "    self.attention = BahdanauAttention(self.dec_units) #атрибут подключит механизм внимания, описанный ранее\n",
        "\n",
        "\n",
        "  def call(self, \n",
        "           x,           # Начальный токен\n",
        "           hidden,      # Состояние  энкодера\n",
        "           enc_output): # Выход энкодера\n",
        "\n",
        "    # Enc_output размеры (batch_size, max_length, hidden_size - размер батча, макс.длина фраз, разм.скр.слоя)\n",
        "    context_vector, attention_weights = self.attention(hidden, enc_output)\n",
        "\n",
        "    # Входящий тензор слова пропускаем через эмбеддинг (получаем размеры batch_size, 1, embedding_dim)\n",
        "    x = self.embedding(x)\n",
        "\n",
        "    # Дальше конкатенируем с вектором контекста (получаем размеры batch_size, 1, embedding_dim + hidden_size)\n",
        "    x = tf.concat([tf.expand_dims(context_vector, 1), x], axis=-1)\n",
        "\n",
        "    # Сконкатенированный вектор передаем  в GRU и получаем выход с декодера и состояние\n",
        "    output, state = self.gru(x)\n",
        "\n",
        "    # Output размеры (batch_size * 1, hidden_size)\n",
        "    output = tf.reshape(output, (-1, output.shape[2]))\n",
        "\n",
        "    # Пропускаем через полносвязный слой\n",
        "    x = self.fc(output) #output размеры (batch_size, vocab)\n",
        "\n",
        "    # Вернем выходную фразу, вектор состояния, веса внимания\n",
        "    return x, state, attention_weights"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-b1yEPu3DPqp",
        "outputId": "e7954e61-68f1-412a-f318-a287b95771bd"
      },
      "source": [
        "# Проверим работу декодера, подав на вход случайный массив с нужной размерностью\n",
        "# Создали декодер с параметрами(размер анг.словаря, размерность эмбеддинга, кол-во нейронов, размер батча)\n",
        "decoder = Decoder(vocab_tar_size, embedding_dim, units, BATCH_SIZE)\n",
        "\n",
        "# Подаём на вход случайный массив с нужной размерностью, состояние и выход с кодировщика\n",
        "sample_decoder_output, _, _ = decoder(tf.random.uniform((256, 1)), sample_hidden, sample_output)\n",
        "print ('Размер выхода с декодера: (размер батча, размер словаря) {}'.format(sample_decoder_output.shape))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Размер выхода с декодера: (размер батча, размер словаря) (256, 5417)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1h7uiC6LDVRK"
      },
      "source": [
        "# Выбираем оптимайзер Adam\n",
        "optimizer = Adam() "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1qw4cgWJlDgW"
      },
      "source": [
        "Наша функция потерь называется  `loss_function` - сначала она уберет из расчетов нулевые элементы в истинной и предсказанной фразе. \n",
        "\n",
        "Длина фразы может быть меньше максимально допустимой или фраза может быть сформирована не полностью. Просто не будем учитывать мусор в конце фразы.\n",
        "\n",
        "Далее применим стандартную для Kerasa функцию потерь  SparseCategoricalCrossentropy. По сравнению CategoricalCrossentropy работает также, но позволяет нам не хранить слова в виде OneHotEncoding, что существенно экономить память.\n",
        "\n",
        "На выходе получаем среднее значение потерь:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7YVxy08qDR-5"
      },
      "source": [
        "# Используем SparseCategoricalCrossentropy, к-я может работать с некатегориальными лейблами\n",
        "loss_object = SparseCategoricalCrossentropy(from_logits=True, reduction='none') # Выбираем функцию потерь\n",
        "\n",
        "def loss_function(real, pred):                       # Запишем функцию потерь, на вход подаем фактический и предсказанный результат\n",
        "  mask = tf.math.logical_not(tf.math.equal(real, 0)) # Найдем маску, которая уберет нулевые значения индексов в конце фразы\n",
        "  loss_ = loss_object(real, pred)                    # Фактические и предсказанные результаты передаем в SparseCategoricalCrossentropy и получаем ошибку\n",
        "  mask = tf.cast(mask, dtype=loss_.dtype)            # Согласуем тип маски с типом потерь\n",
        "  loss_ *= mask                                      # Накидываем \"маску\" которая оставит для работы ненулевые значения\n",
        "  \n",
        "  # Вернем reduce_mean - среднее любого выбранного тензора\n",
        "  return tf.reduce_mean(loss_)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qHr21E05q0NN"
      },
      "source": [
        "# Сохраняем процесс обучения модели чекпоинтами тензорфлоу\n",
        "\n",
        "checkpoint_dir = './training_checkpoints'                                               # Даем ссылку на директорию\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")                                # Добавляем префикс \"ckpt\"\n",
        "checkpoint = tf.train.Checkpoint(optimizer=optimizer, encoder=encoder, decoder=decoder) # Сохраняем состояния/показатели оптимизатора и моделей"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Обучение модели"
      ],
      "metadata": {
        "id": "mn5AhnkJwj7W"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SFb7r1SjYf7i"
      },
      "source": [
        "Создадим функцию для обучения модели. На входе - исходная фраза, конечная фраза, начальное состояния кодера. Подаем сразу батчем. На выходе потери на этом батче"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YCNvA66Jq7nE"
      },
      "source": [
        "@tf.function\n",
        "def train_step(inp,         # Входная фраза\n",
        "               targ,        # Точный перевод\n",
        "               enc_hidden): # Состояния энкодера\n",
        "\n",
        "  # Создаем переменную, в которую будем записывать ошибку\n",
        "  loss = 0                             \n",
        "\n",
        "  # Все операции по вычислению градиента записываются на ленту(tape) и мы получаем к ним доступ\n",
        "  with tf.GradientTape() as tape:\n",
        "\n",
        "    # Передаем тензор и начальное состояние в кодировщик и получим выход и состояние на выходе\n",
        "    enc_output, enc_hidden = encoder(inp, enc_hidden)\n",
        "\n",
        "    # Передадим это состояние декодеру\n",
        "    dec_hidden = enc_hidden\n",
        "\n",
        "    # Передаем в качестве входа в декодер индекс токена \"<start>\"\n",
        "    dec_input = tf.expand_dims([targ_language_tokenizer.word_index['<start>']] * BATCH_SIZE, 1)\n",
        "\n",
        "    # Техника \"Teacher forcing\" - подаем предыдущее выходное слово на вход следущего в декодере. Targ.shape[64, 9]\n",
        "\n",
        "    for t in range(1, targ.shape[1]): #для каждого слова из английской фразы\n",
        "\n",
        "      # Передаем в обработку декодеру начальный токен, состояние на выходе из кодера, и выход из кодера\n",
        "      predictions, dec_hidden, _ = decoder(dec_input, dec_hidden, enc_output) # Получаем от декодера предсказание и обновленное состояние\n",
        "\n",
        "      # Обновляем ошибку для текущих предсказаний\n",
        "      loss += loss_function(targ[:, t], predictions)\n",
        "\n",
        "      # Используем \"Teacher forcing\"\n",
        "      dec_input = tf.expand_dims(targ[:, t], 1)\n",
        "\n",
        "  # Получаем ошибку на батче . Targ.shape[64, 9]. Делим на 9\n",
        "  batch_loss = (loss / int(targ.shape[1])) \n",
        "\n",
        "  # Создаем список переменных, для которых TensorFlow будет вычислять градиенты\n",
        "  variables = encoder.trainable_variables + decoder.trainable_variables # создаем переменные, для которых TensorFlow будет вычислять градиенты\n",
        "\n",
        "  # Отслеживаем градиент\n",
        "  gradients = tape.gradient(loss, variables)\n",
        "\n",
        "  # Корректируем веса\n",
        "  optimizer.apply_gradients(zip(gradients, variables))\n",
        "\n",
        "  # Функция обучения вернет ошибку на батче\n",
        "  return batch_loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8GwOr3sEYiW_"
      },
      "source": [
        "Обучаем сеть. 5 эпох. На каждой эпохе прогоняем весь набор данных через функцию обучения. Считаем лоссы. Сохраняем статистику каждые 10 эпох"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Создаем `tf.data` датасет (Раздел `tf.data.Dataset API` предлагает построить готовый конвейер для обучения моделей)"
      ],
      "metadata": {
        "id": "xg4066kSwxVD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def make_data(path_to_file, num_examples): \n",
        "  \n",
        "  input_tensor, target_tensor, inp_language_tokenizer, targ_language_tokenizer = load_dataset(path_to_file, num_examples)\n",
        "\n",
        "  # Вычислим максимальные длины тензоров для английского и русского языков, используя ранее заданную функцию.\n",
        "  max_length_targ, max_length_inp = max_length(target_tensor), max_length(input_tensor)\n",
        "\n",
        "  # Создаем тренировочную и тестовую выборки по формуле 80/20\n",
        "  input_tensor_train, input_tensor_val, target_tensor_train, target_tensor_val = train_test_split(input_tensor, target_tensor, test_size=0.2)\n",
        "  \n",
        "  print(num_examples, \"пар слов\", len(input_tensor_train), \" - длина input_tensor_train\", len(input_tensor_val), \" - длина input_tensor_val\" )\n",
        "  \n",
        "  # Задаем размер русского словаря\n",
        "  vocab_inp_size = len(inp_language_tokenizer.word_index)+1 \n",
        "\n",
        "  # Задаем размер английского словаря\n",
        "  vocab_tar_size = len(targ_language_tokenizer.word_index)+1 \n",
        "\n",
        "  # Создаём датасет из массивов Numpy(рус и анг тренировочные фразы) со случайной подачей тренировочных сэмплов в процессе обучения\n",
        "  dataset = tf.data.Dataset.from_tensor_slices((input_tensor_train, target_tensor_train)).shuffle(BUFFER_SIZE)\n",
        "\n",
        "  # Передаем в датасет размер батча и указываем, что если в тренировке последний батч окажется неполным, то опустим его (drop_remainder=True)\n",
        "  dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)"
      ],
      "metadata": {
        "id": "faCGB-UMnUoU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Процесс обучения"
      ],
      "metadata": {
        "id": "R24fTuQwAAyc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Определим постоянные \n",
        "\n",
        "BUFFER_SIZE = len(input_tensor_train)                     # Укажем что случайно сэмплировать будем по всей длине обучающейся выборки\n",
        "BATCH_SIZE = 256                                          # Указываем размер батча\n",
        "steps_per_epoch = len(input_tensor_train)//BATCH_SIZE     # Укажем количество шагов в одной эпохе\n",
        "embedding_dim = 256                                       # Размерность эмбеддинга, векторного пространства\n",
        "units = 1024 \n",
        "\n",
        "EPOCHS = 5 # устанавливаем количество эпох\n",
        "num_examples_list = [60000, 80000, 100000] # выберем  60К, 80К, 100К  строк (всего в базе 360 К строк, в которых по паре фраз)\n",
        "path = path_to_file"
      ],
      "metadata": {
        "id": "-3WsBwR7aEJd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loss_per_epoch_list = []\n",
        "df=pd.DataFrame(columns=['train loss'])\n",
        "for num_examples in num_examples_list:\n",
        "  NAME = \"{}-количество пар фраз\".format(num_examples)\n",
        "  create_dataset(path_to_file, num_examples)\n",
        "  load_dataset(path_to_file, num_examples)\n",
        "  make_data(path_to_file, num_examples)\n",
        "  \n",
        "  for epoch in range(EPOCHS): # Цикл по каждой эпохе\n",
        "    start = time.time() # Запомним время начала эпохи\n",
        "\n",
        "    progbar = tf.keras.utils.Progbar(target=steps_per_epoch, stateful_metrics=[\n",
        "                                      'batch_loss'], unit_name='batch')        # Создадим индикатор прогресс обучения\n",
        "\n",
        "    enc_hidden = encoder.initialize_hidden_state() # Задаем начальное состояние на скрытом слое encodera \n",
        "    total_loss = 0                                 # Начальное значение итоговой ошибки\n",
        "\n",
        "    # Для батча, входного и выходного тензора на каждом шаге эпохи\n",
        "    for (batch, (inp, targ)) in enumerate(dataset.take(steps_per_epoch)):\n",
        "      batch_loss = train_step(inp, targ, enc_hidden) # Передадим в функцию тензоры и состояние в кодировщике, обучим и получим ошибку на батче\n",
        "      total_loss += batch_loss                       # Добавим ее в итоговую ошибку\n",
        "      progbar.update(                                # Обновим состояние индикатора обучения\n",
        "              batch + 1, values=[('batch_loss', batch_loss)])\n",
        "\n",
        "\n",
        "    # Каждые 10 эпох будем сохранять чекпоинты\n",
        "    if (epoch + 1) % 10 == 0:\n",
        "      checkpoint.save(file_prefix = checkpoint_prefix)\n",
        "\n",
        "    loss_per_epoch =  total_loss / steps_per_epoch\n",
        "  loss_per_epoch_list.append(loss_per_epoch)\n",
        "    \n",
        "  df=df.append(pd.DataFrame(data={'train loss': np.mean(loss_per_epoch)},index=[NAME]))\n",
        " "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dkR2LwvL4Y9j",
        "outputId": "6b344cfc-1e73-44ac-dbf7-12e1079e0968"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "60000 пар слов 48000  - длина input_tensor_train 12000  - длина input_tensor_val\n",
            "187/187 [==============================] - 49s 182ms/batch - batch_loss: 1.6794\n",
            "187/187 [==============================] - 33s 178ms/batch - batch_loss: 1.3264\n",
            "187/187 [==============================] - 33s 179ms/batch - batch_loss: 1.1047\n",
            "187/187 [==============================] - 34s 180ms/batch - batch_loss: 0.8904\n",
            "187/187 [==============================] - 33s 177ms/batch - batch_loss: 0.6454\n",
            "80000 пар слов 64000  - длина input_tensor_train 16000  - длина input_tensor_val\n",
            "187/187 [==============================] - 34s 180ms/batch - batch_loss: 0.5425\n",
            "187/187 [==============================] - 33s 178ms/batch - batch_loss: 0.3479\n",
            "187/187 [==============================] - 33s 177ms/batch - batch_loss: 0.3022\n",
            "187/187 [==============================] - 33s 177ms/batch - batch_loss: 0.2207\n",
            "187/187 [==============================] - 33s 179ms/batch - batch_loss: 0.1593\n",
            "100000 пар слов 80000  - длина input_tensor_train 20000  - длина input_tensor_val\n",
            "187/187 [==============================] - 33s 179ms/batch - batch_loss: 0.1369\n",
            "187/187 [==============================] - 33s 179ms/batch - batch_loss: 0.1090\n",
            "187/187 [==============================] - 33s 177ms/batch - batch_loss: 0.0944\n",
            "187/187 [==============================] - 34s 180ms/batch - batch_loss: 0.0837\n",
            "187/187 [==============================] - 33s 176ms/batch - batch_loss: 0.0860\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 143
        },
        "id": "G_pFDlnkHXjW",
        "outputId": "f86904ee-637c-4733-ebaf-5ca481411b3a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                            train loss\n",
              "60000-количество пар фраз     0.731304\n",
              "80000-количество пар фраз     0.163865\n",
              "100000-количество пар фраз    0.069845"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-a5436715-3302-421b-92c4-4e866a0fd030\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>train loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>60000-количество пар фраз</th>\n",
              "      <td>0.731304</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>80000-количество пар фраз</th>\n",
              "      <td>0.163865</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>100000-количество пар фраз</th>\n",
              "      <td>0.069845</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-a5436715-3302-421b-92c4-4e866a0fd030')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-a5436715-3302-421b-92c4-4e866a0fd030 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-a5436715-3302-421b-92c4-4e866a0fd030');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 280
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Вывод: Чем больше пар фраз принято в словарь, тем точнее результат"
      ],
      "metadata": {
        "id": "31GGXahhvvAe"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "pdB7jFxHv5tW"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}